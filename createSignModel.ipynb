{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@brief: This is a dataset class to load data from tensor \n",
    "\"\"\"\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, data:torch.tensor):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@brief: This is a function to read data from .mat file into tensor\n",
    "\"\"\"\n",
    "def load_tensor_from_mat(file_location:str):\n",
    "    save_dir = \"dataset_cache\"\n",
    "    cache_name = file_location.split(sep=\"/\")[-1].split(sep=\".\")[0] + \".pt\"\n",
    "    save_path = os.path.join(save_dir, cache_name)\n",
    "    if os.path.exists(save_path):\n",
    "        data = torch.load(save_path)    \n",
    "    else:\n",
    "        if os.path.exists(file_location):\n",
    "            np_data = np.transpose(np.array(h5py.File(name=file_location)[\"Gwl_ud\"])).astype(np.float16)\n",
    "            data = torch.from_numpy(np_data).type(torch.float) # transform array into tensor\n",
    "            if not os.path.exists(save_path):\n",
    "                os.mkdir(save_dir)\n",
    "            torch.save(data, save_path)\n",
    "        else:\n",
    "            raise \"ERROR! The input argument file_location does not exist!\"\n",
    "\n",
    "class SignNetData(Dataset):\n",
    "    def __init__(self, file_location:str, mode:str=\"train\", train_ratio:int=0.7, save_dir:str=\"dataset_cache/\"):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "            @save_dir: it indicates the position to load the matrix before  if exists, otherwise save the matrix into the place\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.file_location = file_location\n",
    "        self.train_ratio = train_ratio\n",
    "        self.mode = mode\n",
    "        self.save_dir = save_dir\n",
    "        cache_name = self.file_location.split(sep=\"/\")[-1].split(sep=\".\")[0] + \".pt\"\n",
    "        self.save_path = os.path.join(save_dir, cache_name)\n",
    "        if os.path.exists(self.save_path):\n",
    "            self.data = torch.load(self.save_path)    \n",
    "        else:\n",
    "            if os.path.exists(self.file_location):\n",
    "                np_data = np.transpose(np.array(h5py.File(name=self.file_location)[\"Gwl_ud\"])).astype(np.float16)\n",
    "                self.data = torch.from_numpy(np_data).type(torch.float) # transform array into tensor\n",
    "                if not os.path.exists(self.save_path):\n",
    "                    os.mkdir(save_dir)\n",
    "                torch.save(self.data, self.save_path)\n",
    "            else:\n",
    "                raise \"ERROR! The input argument file_location does not exist!\"\n",
    "        \n",
    "    def __getitem__(self, idx:int):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, name, in_dim:int, hidden_dim:int, activate_func=F.relu):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.encoder = nn.Linear(in_features=in_dim, out_features=hidden_dim, bias=True)\n",
    "        self.decoder = nn.Linear(in_features=hidden_dim, out_features=in_dim, bias=True)\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activate_func = activate_func\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.activate_func(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.activate_func(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = self.activate_func(x)\n",
    "        return x\n",
    "\n",
    "    def save_output(self, save_dir:str):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        filename = self.name + self.in_dim + \"-\" + self.hidden_dim + \".pth\"\n",
    "        file_pos = os.path.join(save_dir, filename)\n",
    "        torch.save(self.state_dict(), file_pos)\n",
    "\n",
    "    def save_params(self, save_dir:str):\n",
    "        pass\n",
    "\n",
    "    def load_input(self, loading_location:str):\n",
    "        t = torch.load(loading_location)\n",
    "\n",
    "    def load_params(self, save_dir:str):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct stacked autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAutoEncoder(nn.Module):\n",
    "    def __init__(self, *layer_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.sae = []\n",
    "        self.activate_func = torch.tanh\n",
    "        for k,v in kwargs.items():\n",
    "            if k == \"activate_func\":\n",
    "                self.activate_func = v\n",
    "            elif k == \"save_dir\":\n",
    "                self.save_dir = v\n",
    "        \n",
    "        # create autoencoders and store them into self.sae\n",
    "        for idx in range(len(layer_dim)-1):\n",
    "            ae = AutoEncoder(layer_dim[idx], layer_dim[idx+1], self.activate_func)\n",
    "            self.sae.append(ae)\n",
    "        \n",
    "        self.in_dim = layer_dim[0]\n",
    "        self.out_dim = layer_dim[-1]\n",
    "\n",
    "        # initialize layers\n",
    "        self.layer_init()\n",
    "        \n",
    "    def layer_init(self):\n",
    "        for ae in self.sae:\n",
    "            for layer in ae.modules():\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_normal_(layer.weight.data)\n",
    "                elif isinstance(layer, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(layer.weight.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" return the depth of the stacked auto-encoder \"\"\"\n",
    "        return len(self.sae)\n",
    "\n",
    "    # def layer_init(self):\n",
    "    #     try:\n",
    "    #         for ae in self.sae:\n",
    "    #             torch.load()\n",
    "    #     except NameError:\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         for ae in self.sae:\n",
    "    #             for layer in ae.modules():\n",
    "    #                 if isinstance(layer, nn.Linear):\n",
    "    #                     nn.init.xavier_normal_(layer.weight.data)\n",
    "    #                 elif isinstance(layer, nn.Conv2d):\n",
    "    #                     nn.init.kaiming_normal_(layer.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for ae in self.sae:\n",
    "            x = ae.encode(x)\n",
    "        return x\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        for ae in self.sae:\n",
    "            x = ae.encode(x)\n",
    "        for ae in reversed(self.sae):\n",
    "            x = ae.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer-wise training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AETrainer():\n",
    "    def __init__(self, data_pos:str, ae:AutoEncoder):\n",
    "        self.model = ae\n",
    "        self.data_location = data_pos\n",
    "\n",
    "    def train(self, epochs:int):\n",
    "        data = self.load_data(self.data_location)\n",
    "        dataset = TensorDataset(data)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=8, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        for epoch in range(epochs):\n",
    "            for in_rep in dataloader:\n",
    "                out_rep = self.model(in_rep)\n",
    "                loss_score = self.model.loss_func(in_rep, out_rep)\n",
    "                optimizer.zero_grad()\n",
    "                loss_score.backward()\n",
    "                optimizer.step()\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"<<<<<<<<<< epochs: {0}/{1} , loss: {2} >>>>>>>>>>\".format(epoch, epochs, loss_score))\n",
    "\n",
    "\n",
    "    def load_params(self, params_pos:str):\n",
    "        pass\n",
    "\n",
    "    def load_data(self, data_pos:str) -> torch.tensor:\n",
    "        return torch.load(data_pos)\n",
    "\n",
    "    def save(self) -> str:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAETrainer():\n",
    "    def __init__(self, \n",
    "                 model:StackedAutoEncoder, \n",
    "                 dataset:Dataset,\n",
    "                 loss_func:F=F.tanh):\n",
    "        \n",
    "        self.models = model\n",
    "        self.dataset = dataset\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def train(self, epochs:int):\n",
    "        node_rep = self.dataset.data.cuda()\n",
    "        print(\"begin to train stacked autoencoder layer-wisely\")\n",
    "        rep_data = SignNetData(\"dataset\\\\epinions_UD.mat\").data\n",
    "        rep_dataset = TensorDataset(rep_data)\n",
    "        for idx in len(self.models):\n",
    "            print(\"<<<<<<<<<< start to train {0}/{1} auto-encoder >>>>>>>>>>\".format(idx+1, len(self.models)))\n",
    "            ae = self.models.sae[idx]\n",
    "            ae_trainer = AETrainer(ae, rep_dataset)\n",
    "            ae_trainer.train(10)\n",
    "            rep_data = ae(rep_data)\n",
    "            rep_dataset = TensorDataset(rep_data)\n",
    "        torch.save(rep_data, \"final_rep.pt\")\n",
    "\n",
    "        # for ae in self.sae.sae:\n",
    "        #     print(\"the {}/{} autoencoder\".format(ae_cnt,len(self.sae.sae)))\n",
    "        #     optimizer = torch.optim.Adam(ae.parameters(),lr=5e-3)\n",
    "        #     rep_dataset = TensorDataset(node_rep)\n",
    "        #     for epoch in range(epochs):\n",
    "        #         node_loader = DataLoader(rep_dataset, batch_size=8, shuffle=True)\n",
    "        #         print(\"run epoch {0}/{1}\".format(epoch+1, epochs))\n",
    "        #         node_cnt = 0\n",
    "        #         for node in node_loader:\n",
    "        #             reconstruct_node = ae(node)\n",
    "        #             optimizer.zero_grad()\n",
    "        #             loss_score = self.loss_func(reconstruct_node, node)\n",
    "        #             if node_cnt % 800 == 0:\n",
    "        #                 print(\"calculate {0}/{1} node\".format(node_cnt, len(rep_dataset)))\n",
    "        #                 print(\"loss={0}\".format(loss_score))\n",
    "        #             loss_score.backward(retain_graph=True)\n",
    "        #             optimizer.step()\n",
    "        #             node_cnt = node_cnt + 8\n",
    "                \n",
    "        #     node_rep = ae.encode(node_rep)\n",
    "        #     ae_cnt = ae_cnt + 1\n",
    "        #     ae.save()\n",
    "            \n",
    "    def eval(self):\n",
    "        dataloader = TensorDataset(self.dataset.data.cuda())\n",
    "        rep_tensor = torch.zeros(self.sae.in_dim, self.sae.out_dim)\n",
    "        for idx in range(len(dataloader)):\n",
    "            rep_tensor[idx] = self.sae(dataloader[idx])\n",
    "        return rep_tensor\n",
    "\n",
    "    def compute_reconstruct_error(self):\n",
    "        dataloader = TensorDataset(self.dataset.data.cuda())\n",
    "        rep_tensor = torch.zeros(self.sae.in_dim, self.sae.in_dim)\n",
    "        for idx in range(len(dataloader)):\n",
    "            rep_tensor[idx] = self.sae.reconstruct(dataloader[idx])\n",
    "        err = torch.mean((torch.sum(rep_tensor - self.dataset.data)))\n",
    "        return err\n",
    "    \n",
    "    def save(self):\n",
    "        for i in range(len(self.sae.sae)):\n",
    "            filename = \"checkpoint\" + str(i) + \".pth\"\n",
    "            saving_location = os.path.join(self.dataset.save_dir, filename)\n",
    "            torch.save(self.sae.sae.state_dict(), saving_location)\n",
    "\n",
    "    def load(self, load_dir:str):\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test self-defined signed network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructing error = -602628.375\n",
      "<__main__.SAETrainer object at 0x000001E04CC21520>\n",
      "begin to train stacked autoencoder layer-wise\n",
      "the 1/2 autoencoder\n",
      "run epoch 1/1\n",
      "calculate 0/7000 node\n",
      "loss=0.017768435180187225\n",
      "calculate 800/7000 node\n",
      "loss=0.16204749047756195\n",
      "calculate 1600/7000 node\n",
      "loss=0.1533586084842682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5472\\2466433353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msae_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_reconstruct_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reconstructing error = {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msae_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msae_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_reconstruct_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reconstructing error = {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5472\\347074275.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run epoch {0}/{1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mnode_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                     \u001b[0mreconstruct_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3\\Anaconda\\envs\\nn\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3\\Anaconda\\envs\\nn\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3\\Anaconda\\envs\\nn\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Python3\\Anaconda\\envs\\nn\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epinions = SignNetData(file_location=\"dataset/epinions_UD.mat\")\n",
    "    Encoder2Vec = StackedAutoEncoder(7000, 1024, 32, activate_func = torch.tanh)\n",
    "    loss = F.mse_loss\n",
    "    sae_trainer = SAETrainer(Encoder2Vec, epinions, loss)\n",
    "    err = sae_trainer.compute_reconstruct_error()\n",
    "    print(\"reconstructing error = {0}\".format(err))\n",
    "    sae_trainer.train(1)\n",
    "    err = sae_trainer.compute_reconstruct_error()\n",
    "    print(\"reconstructing error = {0}\".format(err))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89b2032c3f28bbfe4949527f3d7807655376bb2d09300d5577d5b00177ab8882"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
