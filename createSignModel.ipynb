{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@brief: This is a dataset class to load data from tensor \n",
    "\"\"\"\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, data:torch.tensor):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx:int):\n",
    "        return idx, self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import h5py\n",
    "\n",
    "\"\"\"\n",
    "@brief: The mat scheme file is in dataset and transform them into dataset_cache and store in dataset_cache directory \n",
    "\"\"\"\n",
    "class SignNetData():\n",
    "    def __init__(self, file_path:str, train_ratio:int=0.7, beta:float=0.1, gamma:float=10):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "            @file_path: define the relative path of data to load\n",
    "            @train_ratio: decide the percentage of data used for training\n",
    "        \"\"\"\n",
    "        save_dir = os.path.split(file_path)[0]\n",
    "        self.file_path = file_path\n",
    "        self.train_ratio = train_ratio\n",
    "        self.full_mat_save_path = os.path.join(save_dir, \"full_\" + os.path.split(file_path)[-1].split(sep=\".\")[0] + \".pt\")\n",
    "        self.train_mat_save_path = os.path.join(save_dir, \"train_\" + os.path.split(file_path)[-1].split(sep=\".\")[0] + \".pt\")\n",
    "        self.validate_mat_save_path = os.path.join(save_dir, \"validate_\" + os.path.split(file_path)[-1].split(sep=\".\")[0] + \".pt\")\n",
    "        self.test_mat_save_path = os.path.join(save_dir, \"test_\" + os.path.split(file_path)[-1].split(sep=\".\")[0] + \".pt\")\n",
    "        self.penalty_mat_save_path = os.path.join(save_dir, \"penalty_\" + os.path.split(file_path)[-1].split(sep=\".\")[0] + \".pt\")\n",
    "\n",
    "        if os.path.exists(self.full_mat_save_path):\n",
    "            full_data = torch.load(self.full_mat_save_path)\n",
    "        else:\n",
    "            full_data = self.load_tensor_from_mat(self.file_path)\n",
    "            torch.save(full_data, self.full_mat_save_path)\n",
    "\n",
    "       \n",
    "        if not (os.path.exists(self.train_mat_save_path) \n",
    "            and os.path.exists(self.validate_mat_save_path) \n",
    "            and os.path.exists(self.penalty_mat_save_path)):\n",
    "\n",
    "            print(\"begin to split dataset\")\n",
    "            nz_idx = torch.nonzero(full_data)\n",
    "            random.shuffle(nz_idx)\n",
    "            num_of_train_sample = int(self.train_ratio * len(nz_idx))\n",
    "            num_of_val_sample = int((len(nz_idx) - num_of_train_sample) / 2)\n",
    "\n",
    "            train_data = torch.zeros(full_data.shape)\n",
    "            validate_data = torch.zeros(full_data.shape)\n",
    "            test_data = torch.zeros(full_data.shape)\n",
    "            penalty_mat = torch.ones(full_data.shape)\n",
    "            print(\"There are {0} edges in the graph.\".format(len(nz_idx)))\n",
    "            \n",
    "            idx_cnt = 0\n",
    "            for coord in nz_idx:\n",
    "                coord = tuple(coord)\n",
    "                if idx_cnt % 100000 == 0:\n",
    "                    print(\"already processed {0}/{1} edges\".format(idx_cnt, len(nz_idx)))\n",
    "                if idx_cnt < num_of_train_sample:\n",
    "                    train_data[coord] = full_data[coord]    \n",
    "                elif num_of_train_sample <= idx_cnt < num_of_train_sample + num_of_val_sample:\n",
    "                    validate_data[coord] = full_data[coord]\n",
    "                else:\n",
    "                    test_data[coord] = full_data[coord]\n",
    "                \n",
    "\n",
    "                if full_data[coord] == 1:\n",
    "                    penalty_mat[coord] = beta\n",
    "                else:\n",
    "                    penalty_mat[coord] = beta * gamma\n",
    "                idx_cnt = idx_cnt + 1\n",
    "\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.mkdir(save_dir)\n",
    "            \n",
    "            torch.save(train_data, self.train_mat_save_path)\n",
    "            torch.save(validate_data, self.validate_mat_save_path)\n",
    "            torch.save(test_data, self.test_mat_save_path)\n",
    "            print(\"self.penalty_mat_save_path = {0}\".format(self.penalty_mat_save_path))\n",
    "            torch.save(penalty_mat, self.penalty_mat_save_path)\n",
    "            print(\"finish split dataset\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    @brief: This is a function to read data from .mat file into tensor\n",
    "    \"\"\"\n",
    "    def load_tensor_from_mat(self, load_path:str)->torch.tensor:\n",
    "        if os.path.exists(load_path):\n",
    "            np_data = np.transpose(np.array(h5py.File(name=load_path)[\"Gwl_ud\"])).astype(np.float16)\n",
    "            data = torch.from_numpy(np_data).type(torch.float) # transform array into tensor\n",
    "            return data\n",
    "        else:\n",
    "            raise \"ERROR! The input argument file_location does not exist!\"\n",
    "\n",
    "    def get_full_data(self):\n",
    "        if os.path.exists(self.full_mat_save_path):\n",
    "            raise \"Error! The full_mat_save_path does not exist!\"\n",
    "        return self.load_tensor_from_mat(self.full_mat_save_path)\n",
    "\n",
    "    def get_train_data(self):\n",
    "        if os.path.exists(self.train_mat_save_path):\n",
    "            raise \"Error! The train_mat_save_path does not exist!\"\n",
    "        return self.load_tensor_from_mat(self.train_mat_save_path)\n",
    "        \n",
    "    def get_validate_data(self):\n",
    "        if os.path.exists(self.validate_mat_save_path):\n",
    "            raise \"Error! The validate_mat_save_path does not exist!\"\n",
    "        return self.load_tensor_from_mat(self.validate_mat_save_path)\n",
    "\n",
    "    def get_test_data(self):\n",
    "        if os.path.exists(self.test_mat_save_path):\n",
    "            raise \"Error! The test_mat_save_path does not exist!\"\n",
    "        return self.load_tensor_from_mat(self.test_mat_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                name, \n",
    "                in_dim:int, \n",
    "                hidden_dim:int, \n",
    "                activate_func=torch.tanh, \n",
    "                params_save_dir:str=\"params_cache\"):\n",
    "        super().__init__()\n",
    "        self.model_name = name\n",
    "        self.file_name = name + \"-\" + str(in_dim) + \"-\" + str(hidden_dim) + \".pth\"\n",
    "        self.params_path = os.path.join(params_save_dir, self.file_name)\n",
    "        self.encoder = nn.Linear(in_features=in_dim, out_features=hidden_dim, bias=True)\n",
    "        self.decoder = nn.Linear(in_features=hidden_dim, out_features=in_dim, bias=True)\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activate_func = activate_func\n",
    "        self.params_save_dir = params_save_dir,\n",
    "        self.params_init()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.activate_func(x)\n",
    "        x = self.decoder(x)\n",
    "        x = self.activate_func(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.activate_func(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = self.activate_func(x)\n",
    "        return x\n",
    "\n",
    "    def save_params(self):\n",
    "        torch.save(self.state_dict(), self.params_path)\n",
    "    \n",
    "    def params_init(self):\n",
    "        if os.path.exists(self.params_path):\n",
    "            self.load_state_dict(torch.load(self.params_path))\n",
    "\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.encoder.weight.data)\n",
    "            nn.init.constant_(self.encoder.bias.data, 0.0)\n",
    "            nn.init.kaiming_normal_(self.decoder.weight.data)\n",
    "            nn.init.constant_(self.decoder.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct stacked autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAutoEncoder():\n",
    "    def __init__(self, *layer_dim, **kwargs):\n",
    "        print(\"begin to init stacked auto encoder\")\n",
    "        self.sae = []\n",
    "        self.activate_func = torch.tanh\n",
    "        self.params_dir = \"params_cache\"\n",
    "        self.rep_save_dir = \"rep_cache\"\n",
    "        self.name = \"untitled\"\n",
    "        \n",
    "        for k,v in kwargs.items():\n",
    "            if k == \"activate_func\":\n",
    "                self.activate_func = v\n",
    "            elif k == \"params_load_dir\":\n",
    "                self.params_load_dir = v\n",
    "            elif k == \"rep_save_dir\":\n",
    "                self.rep_save_dir = v\n",
    "            elif k == \"name\":\n",
    "                self.name = v\n",
    "\n",
    "        # create autoencoders and store them into self.sae\n",
    "        for idx in range(len(layer_dim)-1):\n",
    "            ae_name = self.name + \"-AE\" + str(idx)\n",
    "            ae = AutoEncoder(ae_name, layer_dim[idx], layer_dim[idx+1], self.activate_func, self.params_dir)\n",
    "            self.sae.append(ae)\n",
    "        \n",
    "        self.in_dim = layer_dim[0]\n",
    "        self.out_dim = layer_dim[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" return the depth of the stacked auto-encoder \"\"\"\n",
    "        return len(self.sae)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for ae in self.sae:\n",
    "            x = ae.encode(x)\n",
    "        return x\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        for ae in self.sae:\n",
    "            x = ae.encode(x)\n",
    "        for ae in reversed(self.sae):\n",
    "            x = ae.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer-wise training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AETrainer():\n",
    "    def __init__(self, ae:AutoEncoder, input_rep_path:str, penalty:torch.tensor, rep_save_dir = \"rep_cache\"):\n",
    "        print(\"begin to init AE trainer\")\n",
    "        self.model = ae\n",
    "        self.penalty = penalty\n",
    "        self.input_rep_path = input_rep_path\n",
    "        rep_out_name = ae.model_name + \"-\" +str(ae.hidden_dim) + \"-rep\" + \".pt\"\n",
    "        self.save_rep_path = os.path.join(rep_save_dir, rep_out_name) \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "\n",
    "    def train(self, epochs:int):\n",
    "        rep = torch.load(self.input_rep_path)\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        rep.to(device)\n",
    "        dataset = TensorDataset(rep)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=8, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        node_cnt = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(\"<< epochs: {0}/{1} >>\".format(epoch+1, epochs))\n",
    "            node_cnt = 0\n",
    "            for idx, in_rep in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                out_rep = self.model(in_rep)\n",
    "                if self.penalty[idx].shape == out_rep.shape:\n",
    "                    out_rep = self.penalty[idx] * self.penalty[idx] * out_rep\n",
    "                    in_rep = self.penalty[idx] * self.penalty[idx] * in_rep\n",
    "                loss_score = F.l1_loss(in_rep, out_rep)\n",
    "                loss_score.backward()\n",
    "                optimizer.step()\n",
    "                node_cnt = node_cnt + 8\n",
    "                if node_cnt % 800 == 0:\n",
    "                    print(\"calculate nodes {0}/{1} \".format(node_cnt, len(dataset)))\n",
    "        \n",
    "        nodes_rep = torch.zeros(len(dataset), self.model.hidden_dim).cuda()\n",
    "        for idx, in_rep in dataloader:\n",
    "            nodes_rep[idx] = self.model.encode(in_rep)\n",
    "\n",
    "        if not os.path.exists(os.path.split(self.save_rep_path)[0]):\n",
    "            os.mkdir(os.path.split(self.save_rep_path)[0])\n",
    "        torch.save(nodes_rep, self.save_rep_path)\n",
    "\n",
    "        print(\"Saving auto-encoder {0}'s parameters to file {1}\".format(self.model.model_name, self.save_rep_path))\n",
    "        self.model.save_params()\n",
    "        torch.cuda.empty_cache()\n",
    "        del self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAETrainer():\n",
    "    def __init__(self, sae:StackedAutoEncoder, data_path:str):\n",
    "        self.models = sae\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def train(self, epochs:int, mode:str=\"train\"):\n",
    "        sign_net = SignNetData(self.data_path)\n",
    "        if mode == \"train\":\n",
    "            rep_path = sign_net.train_mat_save_path\n",
    "        elif mode == \"validate\":\n",
    "            rep_path = sign_net.validate_mat_save_path\n",
    "        elif mode == \"test\":\n",
    "            rep_path = sign_net.test_mat_save_path\n",
    "        else:\n",
    "            raise \"Error! Argument mode is illegal!\"\n",
    "        \n",
    "        self.data_path = rep_path\n",
    "        penalty = torch.load(sign_net.penalty_mat_save_path).cuda()\n",
    "\n",
    "        for model in self.models.sae:\n",
    "            print(\"<<<<<<<<<< begin to  train model: {0} >>>>>>>>>>\".format(model.model_name))\n",
    "            ae_trainer =  AETrainer(model, rep_path, penalty)\n",
    "            ae_trainer.train(epochs)\n",
    "            rep_path = ae_trainer.save_rep_path\n",
    "        return rep_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import DGL modules for further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as dglfn\n",
    "from dgl.data import DGLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignNetGraph(DGLDataset):\n",
    "    def __init__(self, mat_path:str, feat_path:str)->None: \n",
    "        dataset_dir, mat_name = os.path.split(mat_path)\n",
    "        self.feat_path = feat_path\n",
    "        super().__init__(name=mat_name, raw_dir=dataset_dir, save_dir=dataset_dir)\n",
    "\n",
    "\n",
    "    def process(self) -> None:\n",
    "        # read data from file to generate Dataset\n",
    "        torch_data = torch.load(self.raw_path)\n",
    "        src, dst = torch.nonzero(torch_data, as_tuple=True)\n",
    "        self.graph = dgl.graph((src,dst), num_nodes=torch_data.shape[0])\n",
    "        self.graph.edata[\"sign\"] = torch_data[src,dst]\n",
    "        self.graph.ndata[\"feat\"] = torch.load(self.feat_path).cpu()\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.graph.to(device)\n",
    "\n",
    "    def __getitem__(self, idx)->None:\n",
    "        return self.graph.ndata[\"feat\"][idx]\n",
    "    \n",
    "    def __len__(self)-> None:\n",
    "        return self.graph.num_nodes()\n",
    "\n",
    "    def save(self)->None:\n",
    "        # save graph\n",
    "        graph_path = os.path.join(self.save_dir, self.name.split(\".\")[0] + '_dgl_graph.bin')\n",
    "        dgl.save_graphs(graph_path, self.graph)\n",
    "\n",
    "\n",
    "    def load(self)->None:\n",
    "        # load processed data from directory `self.save_path`\n",
    "        graph_path = os.path.join(self.save_dir, self.name.split(\".\")[0] + '_dgl_graph.bin')\n",
    "        graphs, _ = dgl.load_graphs(graph_path)\n",
    "        self.graph = graphs[0]\n",
    "\n",
    "    def has_cache(self)->bool:\n",
    "        # check whether there are processed data in `self.save_path`\n",
    "        graph_path = os.path.join(self.save_path, self.name.split(\".\")[0] + '_dgl_graph.bin')\n",
    "        return os.path.exists(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2Vec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph.apply_edges(func=dglfn.v_sub_u(\"feat\", \"feat\", \"diff\"))\n",
    "    \n",
    "    def comp_vec(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2VTrainer():\n",
    "    def __init__(self, e2v:Encoder2Vec, graph_path:str, feat_path:str):\n",
    "        self.model = e2v\n",
    "        self.graph_path = graph_path\n",
    "        self.graph = SignNetGraph(graph_path, feat_path)\n",
    "\n",
    "    def train(self, epochs:int):\n",
    "        dataloader = dgl.dataloading.pytorch.EdgeDataLoader(self.graph)\n",
    "        for epoch in range(epochs):\n",
    "            for src, dst  in dataloader:\n",
    "                with self.graph.local_scope():\n",
    "                    pass\n",
    "        dgl.save_graphs(self.graph_path, self.graph)\n",
    "        return self.graph_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatEvaluator():\n",
    "    def __init__(self, graph_path):\n",
    "        self.graph_path = graph_path\n",
    "        graphs, _ = dgl.load_graphs(graph_path)\n",
    "        self.graph = graphs[0]\n",
    "\n",
    "    def link_predict(self):\n",
    "        # target paper uses AUC(area under curve) and AP(average precision) as indicators to test model for link prediction\n",
    "        auc = 0.0\n",
    "        ap = 0.0\n",
    "        return (auc, ap)\n",
    "\n",
    "    def node_cluster(self):\n",
    "        # target paper uses error rate to test model for node clustering task\n",
    "        # the ground truth is the result of k-mean on the same dataset\n",
    "        err_rate = 0.0\n",
    "        return err_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test self-defined signed network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    original_data_path = \"dataset/slashdot_UD.mat\"\n",
    "    model_name = \"slash\"\n",
    "    sae = StackedAutoEncoder(7000, 1024, 128, 32, name=model_name)\n",
    "    sae_trainer = SAETrainer(sae, original_data_path)\n",
    "    feat_path = sae_trainer.train(1, mode=\"train\")\n",
    "    e2v_trainer = E2VTrainer(original_data_path, feat_path)\n",
    "    graph_path = e2v_trainer.train(10)\n",
    "    evaluator = FeatEvaluator(graph_path)\n",
    "    auc, ap = evaluator.link_predict()\n",
    "    print(\"<<<<<<<<<< Final Result of the Encoder2Vec >>>>>>>>>>>\")\n",
    "    print(\"link prediction performance: AUC={1}, AP={2}\".format(original_data_path, auc, ap))\n",
    "    err_rate =  evaluator.node_cluster()\n",
    "    print(\"node clustering performance: error rate = {0}\".format(err_rate))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89b2032c3f28bbfe4949527f3d7807655376bb2d09300d5577d5b00177ab8882"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
